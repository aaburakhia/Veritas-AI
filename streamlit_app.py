import streamlit as st
import joblib
import spacy
import pandas as pd

# --- Page Configuration ---
st.set_page_config(
    page_title="Veritas AI | The Truth Detector",
    page_icon="üîé",
    layout="wide"
)

# --- Constants ---
MIN_CHARS = 250

# --- Caching the Models ---
@st.cache_resource
def load_models():
    try:
        model = joblib.load('model/best_random_forest_model.joblib')
        vectorizer = joblib.load('model/tfidf_vectorizer.joblib')
        nlp = spacy.load("en_core_web_sm")
        print("‚úÖ Models loaded successfully.")
        return model, vectorizer, nlp
    except Exception as e:
        print(f"‚ùå Error loading files: {e}")
        st.error(f"Error loading model files: {e}")
        return None, None, None

model, vectorizer, nlp = load_models()

# --- Preprocessing Function ---
def preprocess_text(text):
    doc = nlp(text)
    processed_tokens = [
        token.lemma_.lower() 
        for token in doc 
        if not token.is_stop and not token.is_punct
    ]
    return " ".join(processed_tokens)

# --- Sidebar ---
st.sidebar.header("About Veritas AI üîé")
st.sidebar.write("""
This application uses a machine learning model to distinguish between human-written and AI-generated text.

It is powered by a **Random Forest** model with **99.77% accuracy** on its test dataset.
""")
st.sidebar.warning(f"**Note:** The model is most accurate with texts longer than {MIN_CHARS} characters, as it was trained on long-form essays.")
st.sidebar.markdown("---")
st.sidebar.write("Project by: [Your Name Here]") # <-- Change this to your name!

# --- Main App Interface ---
st.title("Veritas AI: The Truth Detector")
st.write("An advanced AI detector to determine if a text was written by a human or generated by AI.")
st.markdown("---")

# --- Centered Layout ---
# We create three columns; the middle one will contain our app.
# This is a common trick to create a cleaner, centered layout.
col1, col2, col3 = st.columns([1, 3, 1])

with col2:
    # We use a key to store the text_area's content in st.session_state
    input_text = st.text_area(
        "Paste your text here to begin analysis:", 
        height=300, 
        key="user_input" # This is the key to fixing the bug
    )

    # The character count is now reliably read from session_state
    char_count = len(st.session_state.user_input)
    
    # Display the character counter
    if char_count < MIN_CHARS:
        st.warning(f"Characters: {char_count}/{MIN_CHARS}")
    else:
        st.success(f"Characters: {char_count}/{MIN_CHARS}")

    # The "Analyze" button, centered and prominent
    analyze_button = st.button("Detect AI", use_container_width=True, type="primary", disabled=(char_count < MIN_CHARS))

    st.markdown("---")

    # --- Results Section ---
    if analyze_button:
        if model is not None and vectorizer is not None:
            with st.spinner("Analyzing..."):
                processed_text = preprocess_text(st.session_state.user_input)
                vectorized_text = vectorizer.transform([processed_text])
                prediction = model.predict(vectorized_text)[0]
                probabilities = model.predict_proba(vectorized_text)[0]

                if prediction == 1:
                    label = "AI-Generated"
                    confidence = probabilities[1]
                    st.error(f"## **Result: {label} ü§ñ**")
                else:
                    label = "Human-Written"
                    confidence = probabilities[0]
                    st.success(f"## **Result: {label} üßë‚Äçüíª**")
                
                st.metric(label="Confidence", value=f"{confidence:.2%}")
                st.progress(confidence)
                
                with st.expander("View Processed Text"):
                    st.code(processed_text, language=None)
        else:
            st.error("Model files are not loaded. Please check the logs.")
    else:
        st.info("Results will be displayed here after analysis.")