{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7379779,"sourceType":"datasetVersion","datasetId":4288635},{"sourceId":12623302,"sourceType":"datasetVersion","datasetId":7975711}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \"spacy\" \"numpy<2.0\" \"scikit-learn\" \"pandas\" \"tqdm\"\n!python -m spacy download en_core_web_sm\n\nprint(\"Libraries installed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:30:55.436411Z","iopub.execute_input":"2025-08-13T16:30:55.436572Z","iopub.status.idle":"2025-08-13T16:31:13.628896Z","shell.execute_reply.started":"2025-08-13T16:30:55.436556Z","shell.execute_reply":"2025-08-13T16:31:13.628124Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nLibraries installed successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy.tokens import DocBin\nfrom spacy.training import Example\nfrom tqdm.auto import tqdm\nimport os\nimport shutil\nimport time\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nprint(\"Loading and preparing datasets...\")\n\ndef load_and_prepare_data(sample_fraction=0.15):\n    \"\"\"Load and prepare data\"\"\"\n    try:\n        # Dataset paths\n        path_daigt = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv'\n        path_llm = '/kaggle/input/llm-detect-ai-generated-text-dataset/Training_Essay_Data.csv' \n        path_ai_human = '/kaggle/input/ai-vs-human-text/AI_Human.csv'\n        \n        print(f\"   Loading {sample_fraction*100}% of data for faster training...\")\n        \n        print(\"   Loading DAIGT dataset...\")\n        df_daigt = pd.read_csv(path_daigt, nrows=int(119757 * sample_fraction))\n        df_daigt = df_daigt[['text', 'label']].dropna()\n        print(f\"     DAIGT samples: {len(df_daigt):,}\")\n        \n        print(\"   Loading LLM dataset...\")\n        df_llm = pd.read_csv(path_llm, nrows=int(44868 * sample_fraction))\n        df_llm = df_llm.rename(columns={'generated': 'label'})[['text', 'label']].dropna()\n        print(f\"     LLM samples: {len(df_llm):,}\")\n        \n        print(\"   Loading AI-Human dataset...\")\n        df_ai_human = pd.read_csv(path_ai_human, nrows=int(200000 * sample_fraction))\n        df_ai_human = df_ai_human.rename(columns={'generated': 'label'})\n        df_ai_human = df_ai_human[['text', 'label']].dropna()\n        df_ai_human['label'] = df_ai_human['label'].astype(int)\n        print(f\"     AI-Human samples: {len(df_ai_human):,}\")\n        \n        print(\"   Combining datasets...\")\n        combined_df = pd.concat([df_daigt, df_llm, df_ai_human], ignore_index=True)\n        print(f\"     Combined total: {len(combined_df):,}\")\n        \n        # Remove duplicates\n        combined_df = combined_df.drop_duplicates(subset='text').reset_index(drop=True)\n        print(f\"     After deduplication: {len(combined_df):,}\")\n        \n        # Balance dataset for better training\n        print(\"   • Balancing dataset...\")\n        df_human = combined_df[combined_df.label == 0]  # Human text\n        df_ai = combined_df[combined_df.label == 1]     # AI text\n        \n        print(f\"     Before balancing - Human: {len(df_human):,}, AI: {len(df_ai):,}\")\n        \n      \n        min_count = min(len(df_human), len(df_ai))\n        max_per_class = min_count \n        \n        df_human_balanced = df_human.sample(n=max_per_class, random_state=42)\n        df_ai_balanced = df_ai.sample(n=max_per_class, random_state=42)\n        \n        # Combine balanced data\n        final_df = pd.concat([df_human_balanced, df_ai_balanced]).reset_index(drop=True)\n        final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n        \n        print(f\"Final dataset prepared: {len(final_df):,} samples\")\n        print(f\"   Human text: {(final_df.label==0).sum():,}\")\n        print(f\"   AI text: {(final_df.label==1).sum():,}\")\n        print(f\"   Balance ratio: {(final_df.label==1).sum()/(final_df.label==0).sum():.3f}\")\n        \n        return final_df\n        \n    except FileNotFoundError as e:\n        print(f\"❌ Dataset file not found: {e}\")\n        print(\"   Please ensure all datasets are added to your Kaggle notebook.\")\n        return None\n    except Exception as e:\n        print(f\"❌ Data loading failed: {e}\")\n        return None\n\n# Execute data loading\ndf = load_and_prepare_data(sample_fraction = 0.8)\n\nif df is None:\n    raise Exception(\"❌ Data loading failed! Please check dataset paths.\")\n    \nprint(f\"\\nData loading completed successfully!\")\nprint(f\"Ready for training with {len(df):,} balanced samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:32:04.526322Z","iopub.execute_input":"2025-08-13T16:32:04.526652Z","iopub.status.idle":"2025-08-13T16:32:12.184803Z","shell.execute_reply.started":"2025-08-13T16:32:04.526603Z","shell.execute_reply":"2025-08-13T16:32:12.183932Z"}},"outputs":[{"name":"stdout","text":"Loading and preparing datasets...\n   Loading 80.0% of data for faster training...\n   Loading DAIGT dataset...\n     DAIGT samples: 44,868\n   Loading LLM dataset...\n     LLM samples: 29,145\n   Loading AI-Human dataset...\n     AI-Human samples: 160,000\n   Combining datasets...\n     Combined total: 234,013\n     After deduplication: 166,678\n   • Balancing dataset...\n     Before balancing - Human: 91,889, AI: 74,789\nFinal dataset prepared: 149,578 samples\n   Human text: 74,789\n   AI text: 74,789\n   Balance ratio: 1.000\n\nData loading completed successfully!\nReady for training with 149,578 balanced samples.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def train_spacy_classifier_FIXED(df, max_samples_per_class=6000):\n    \"\"\"\n    SpaCy text classifier\n    This version addresses the model collapse issue from your original code\n    \"\"\"\n    \n    print(\"SpaCy Training Started\")\n    print(\"=\" * 50)\n    \n    # Prepare balanced dataset\n    df_human = df[df.label == 0].sample(\n        n=min(max_samples_per_class, len(df[df.label == 0])), \n        random_state=42\n    )\n    df_ai = df[df.label == 1].sample(\n        n=min(max_samples_per_class, len(df[df.label == 1])), \n        random_state=42\n    )\n    \n    balanced_df = pd.concat([df_human, df_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    print(f\"Training Configuration:\")\n    print(f\"   Total samples: {len(balanced_df):,}\")\n    print(f\"   Human samples: {len(df_human):,}\")\n    print(f\"   AI samples: {len(df_ai):,}\")\n    print(f\"   Perfect balance: {len(df_human) == len(df_ai)}\")\n    \n    # Split data (80% train, 20% test)\n    X_train, X_test, y_train, y_test = train_test_split(\n        balanced_df['text'], balanced_df['label'], \n        test_size=0.2, random_state=42, stratify=balanced_df['label']\n    )\n    \n    print(f\"   Training samples: {len(X_train):,}\")\n    print(f\"   Testing samples: {len(X_test):,}\")\n    \n    # Verify train/test balance\n    train_balance = pd.Series(y_train).value_counts().sort_index()\n    test_balance = pd.Series(y_test).value_counts().sort_index()\n    print(f\"   Train balance - Human: {train_balance[0]}, AI: {train_balance[1]}\")\n    print(f\"   Test balance - Human: {test_balance[0]}, AI: {test_balance[1]}\")\n    \n    # Create SpaCy model (blank English model)\n    print(\"\\n Creating SpaCy model...\")\n    nlp = spacy.blank(\"en\")\n    \n    # Add text classifier with FIXED configuration\n    textcat = nlp.add_pipe(\"textcat\", config={\n        \"model\": {\n            \"@architectures\": \"spacy.TextCatBOW.v3\",  # Use v3 (more stable)\n            \"exclusive_classes\": True,  # Either AI or Human, not both\n            \"ngram_size\": 2,  # Use bigrams for better feature extraction\n            \"no_output_layer\": False\n        }\n    })\n    \n    # Add labels (CRITICAL: Use string labels, not numeric)\n    textcat.add_label(\"HUMAN\")  # Label for human text\n    textcat.add_label(\"AI\")     # Label for AI text\n    \n    print(\"   • Architecture: TextCatBOW.v3 with exclusive classification\")\n    print(\"   • Features: Bigrams (ngram_size=2)\")\n    print(\"   • Labels: HUMAN, AI\")\n    \n    # Prepare training examples (FIXED format)\n    print(\"\\nPreparing training examples...\")\n    train_examples = []\n    \n    for i, (text, label) in enumerate(zip(X_train, y_train)):\n        # Create document\n        doc = nlp.make_doc(str(text))\n        \n        # Create categories (CRITICAL: Proper format for exclusive classification)\n        cats = {\n            \"HUMAN\": 1.0 if label == 0 else 0.0, \n            \"AI\": 1.0 if label == 1 else 0.0\n        }\n        \n        # Create training example\n        example = Example.from_dict(doc, {\"cats\": cats})\n        train_examples.append(example)\n        \n        if (i + 1) % 1000 == 0:\n            print(f\"   • Prepared {i+1:,} examples...\")\n    \n    print(f\"{len(train_examples):,} training examples prepared\")\n    \n    # Initialize model (CRITICAL: Must do this before training)\n    print(\"\\nInitializing model...\")\n    nlp.initialize()\n    print(\"   Model initialized successfully\")\n    \n    # FIXED Training loop\n    print(\"\\nStarting training loop...\")\n    \n    # Training parameters (optimized for accuracy and speed)\n    n_epochs = 10          # Sufficient epochs for convergence\n    batch_size = 32        # Smaller batches for better gradient updates\n    dropout_rate = 0.3     # Prevent overfitting\n    learning_rate = 0.001  # Conservative learning rate\n    \n    print(f\"   • Epochs: {n_epochs}\")\n    print(f\"   • Batch size: {batch_size}\")\n    print(f\"   • Dropout rate: {dropout_rate}\")\n    print(f\"   • Learning rate: {learning_rate}\")\n    \n    # Training loop with progress tracking\n    for epoch in range(n_epochs):\n        epoch_start = time.time()\n        losses = {}\n        \n        # Shuffle training examples each epoch\n        train_examples_shuffled = train_examples.copy()\n        np.random.shuffle(train_examples_shuffled)\n        \n        # Process in batches\n        n_batches = len(train_examples_shuffled) // batch_size\n        \n        for i in range(0, len(train_examples_shuffled), batch_size):\n            batch = train_examples_shuffled[i:i+batch_size]\n            nlp.update(batch, drop=dropout_rate, losses=losses)\n        \n        epoch_time = time.time() - epoch_start\n        \n        # Evaluate every 2 epochs\n        if (epoch + 1) % 2 == 0:\n            # Quick accuracy check on sample\n            correct = 0\n            total = 0\n            sample_size = min(200, len(X_test))\n            sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n            \n            for idx in sample_indices:\n                doc = nlp(str(X_test.iloc[idx]))\n                \n                # Get prediction (FIXED: Check which class has higher score)\n                ai_score = doc.cats[\"AI\"]\n                human_score = doc.cats[\"HUMAN\"]\n                predicted = 1 if ai_score > human_score else 0\n                actual = y_test.iloc[idx]\n                \n                if predicted == actual:\n                    correct += 1\n                total += 1\n            \n            accuracy = correct / total\n            loss_value = losses.get('textcat', 0)\n            \n            print(f\"   📊 Epoch {epoch+1:2d}: Loss={loss_value:.4f}, \"\n                  f\"Accuracy={accuracy:.4f} ({accuracy*100:.1f}%), Time={epoch_time:.1f}s\")\n        else:\n            loss_value = losses.get('textcat', 0)\n            print(f\"   📊 Epoch {epoch+1:2d}: Loss={loss_value:.4f}, Time={epoch_time:.1f}s\")\n        \n        # Memory cleanup\n        gc.collect()\n    \n    print(\"Training completed successfully!\")\n    \n    # Full evaluation on test set\n    print(f\"\\nFinal Model Evaluation...\")\n    print(\"   • Running full test set evaluation...\")\n    \n    predictions = []\n    confidences_ai = []\n    confidences_human = []\n    \n    for text in tqdm(X_test, desc=\"Evaluating model\", leave=False):\n        doc = nlp(str(text))\n        \n        ai_confidence = doc.cats[\"AI\"]\n        human_confidence = doc.cats[\"HUMAN\"]\n        \n        # Prediction based on higher confidence\n        predicted = 1 if ai_confidence > human_confidence else 0\n        \n        predictions.append(predicted)\n        confidences_ai.append(ai_confidence)\n        confidences_human.append(human_confidence)\n    \n    # Calculate final metrics\n    final_accuracy = accuracy_score(y_test, predictions)\n    \n    print(f\"\\nFINAL EVALUATION RESULTS:\")\n    print(f\"=\" * 40)\n    print(f\"Overall Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n    print(f\"Test Samples: {len(X_test):,}\")\n    \n    # Detailed classification report\n    print(f\"\\nDetailed Classification Report:\")\n    print(classification_report(y_test, predictions, \n                              target_names=['Human', 'AI'], \n                              digits=4))\n    \n    # Confusion matrix (FIXED: This should now show predictions for both classes)\n    cm = confusion_matrix(y_test, predictions)\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"                 Predicted\")\n    print(f\"Actual       Human      AI\")\n    print(f\"Human     {cm[0,0]:8d}  {cm[0,1]:8d}\")\n    print(f\"AI        {cm[1,0]:8d}  {cm[1,1]:8d}\")\n    \n    # Confidence statistics\n    print(f\"\\n📈 Confidence Statistics:\")\n    print(f\"   • Mean AI confidence: {np.mean(confidences_ai):.4f}\")\n    print(f\"   • Mean Human confidence: {np.mean(confidences_human):.4f}\")\n    print(f\"   • Std AI confidence: {np.std(confidences_ai):.4f}\")\n    print(f\"   • Std Human confidence: {np.std(confidences_human):.4f}\")\n    \n    return nlp, final_accuracy, cm, X_test, y_test\n\nprint(\"Training function defined and ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:32:26.625081Z","iopub.execute_input":"2025-08-13T16:32:26.625360Z","iopub.status.idle":"2025-08-13T16:32:26.645747Z","shell.execute_reply.started":"2025-08-13T16:32:26.625337Z","shell.execute_reply":"2025-08-13T16:32:26.645075Z"}},"outputs":[{"name":"stdout","text":"Training function defined and ready!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Training configuration\nMAX_SAMPLES_PER_CLASS = 74789 # from the previous cell\nprint(f\"Training Configuration:\")\nprint(f\"   Max samples per class: {MAX_SAMPLES_PER_CLASS:,}\")\nprint(f\"   Total training samples: ~{MAX_SAMPLES_PER_CLASS * 2:,}\")\nprint(f\"   Estimated training time: 20-30 minutes\")\n\n# Execute training\ntraining_start_time = time.time()\n\ntry:\n    print(f\"\\nTraining started at: {time.strftime('%H:%M:%S')}\")\n    \n    model, final_accuracy, conf_matrix, X_test, y_test = train_spacy_classifier_FIXED(\n        df, \n        max_samples_per_class=MAX_SAMPLES_PER_CLASS\n    )\n    \n    total_training_time = time.time() - training_start_time\n    \n    print(f\"\\nSUCCESS! Training completed!\")\n    print(f\"=\" * 40)\n    print(f\"Total training time: {total_training_time/60:.1f} minutes\")\n    print(f\"Final accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n    print(f\"Model is ready for use!\")\n    \n    # Save the model\n    model_save_path = \"./spacy_ai_detector_WORKING\"\n    model.to_disk(model_save_path)\n    print(f\"Model saved to: {model_save_path}\")\n    \n    # Mark training as successful\n    TRAINING_SUCCESSFUL = True\n    TRAINED_MODEL = model\n    \nexcept Exception as e:\n    print(f\"\\nTraining failed with error: {e}\")\n    print(f\"Error details:\")\n    import traceback\n    traceback.print_exc()\n    TRAINING_SUCCESSFUL = False\n    TRAINED_MODEL = None\n\nprint(f\"\\n🏁 Training process completed at: {time.strftime('%H:%M:%S')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:32:37.681210Z","iopub.execute_input":"2025-08-13T16:32:37.681790Z","iopub.status.idle":"2025-08-13T16:50:43.673394Z","shell.execute_reply.started":"2025-08-13T16:32:37.681767Z","shell.execute_reply":"2025-08-13T16:50:43.672684Z"}},"outputs":[{"name":"stdout","text":"Training Configuration:\n   Max samples per class: 74,789\n   Total training samples: ~149,578\n   Estimated training time: 20-30 minutes\n\nTraining started at: 16:32:37\nSpaCy Training Started\n==================================================\nTraining Configuration:\n   Total samples: 149,578\n   Human samples: 74,789\n   AI samples: 74,789\n   Perfect balance: True\n   Training samples: 119,662\n   Testing samples: 29,916\n   Train balance - Human: 59831, AI: 59831\n   Test balance - Human: 14958, AI: 14958\n\n Creating SpaCy model...\n   • Architecture: TextCatBOW.v3 with exclusive classification\n   • Features: Bigrams (ngram_size=2)\n   • Labels: HUMAN, AI\n\nPreparing training examples...\n   • Prepared 1,000 examples...\n   • Prepared 2,000 examples...\n   • Prepared 3,000 examples...\n   • Prepared 4,000 examples...\n   • Prepared 5,000 examples...\n   • Prepared 6,000 examples...\n   • Prepared 7,000 examples...\n   • Prepared 8,000 examples...\n   • Prepared 9,000 examples...\n   • Prepared 10,000 examples...\n   • Prepared 11,000 examples...\n   • Prepared 12,000 examples...\n   • Prepared 13,000 examples...\n   • Prepared 14,000 examples...\n   • Prepared 15,000 examples...\n   • Prepared 16,000 examples...\n   • Prepared 17,000 examples...\n   • Prepared 18,000 examples...\n   • Prepared 19,000 examples...\n   • Prepared 20,000 examples...\n   • Prepared 21,000 examples...\n   • Prepared 22,000 examples...\n   • Prepared 23,000 examples...\n   • Prepared 24,000 examples...\n   • Prepared 25,000 examples...\n   • Prepared 26,000 examples...\n   • Prepared 27,000 examples...\n   • Prepared 28,000 examples...\n   • Prepared 29,000 examples...\n   • Prepared 30,000 examples...\n   • Prepared 31,000 examples...\n   • Prepared 32,000 examples...\n   • Prepared 33,000 examples...\n   • Prepared 34,000 examples...\n   • Prepared 35,000 examples...\n   • Prepared 36,000 examples...\n   • Prepared 37,000 examples...\n   • Prepared 38,000 examples...\n   • Prepared 39,000 examples...\n   • Prepared 40,000 examples...\n   • Prepared 41,000 examples...\n   • Prepared 42,000 examples...\n   • Prepared 43,000 examples...\n   • Prepared 44,000 examples...\n   • Prepared 45,000 examples...\n   • Prepared 46,000 examples...\n   • Prepared 47,000 examples...\n   • Prepared 48,000 examples...\n   • Prepared 49,000 examples...\n   • Prepared 50,000 examples...\n   • Prepared 51,000 examples...\n   • Prepared 52,000 examples...\n   • Prepared 53,000 examples...\n   • Prepared 54,000 examples...\n   • Prepared 55,000 examples...\n   • Prepared 56,000 examples...\n   • Prepared 57,000 examples...\n   • Prepared 58,000 examples...\n   • Prepared 59,000 examples...\n   • Prepared 60,000 examples...\n   • Prepared 61,000 examples...\n   • Prepared 62,000 examples...\n   • Prepared 63,000 examples...\n   • Prepared 64,000 examples...\n   • Prepared 65,000 examples...\n   • Prepared 66,000 examples...\n   • Prepared 67,000 examples...\n   • Prepared 68,000 examples...\n   • Prepared 69,000 examples...\n   • Prepared 70,000 examples...\n   • Prepared 71,000 examples...\n   • Prepared 72,000 examples...\n   • Prepared 73,000 examples...\n   • Prepared 74,000 examples...\n   • Prepared 75,000 examples...\n   • Prepared 76,000 examples...\n   • Prepared 77,000 examples...\n   • Prepared 78,000 examples...\n   • Prepared 79,000 examples...\n   • Prepared 80,000 examples...\n   • Prepared 81,000 examples...\n   • Prepared 82,000 examples...\n   • Prepared 83,000 examples...\n   • Prepared 84,000 examples...\n   • Prepared 85,000 examples...\n   • Prepared 86,000 examples...\n   • Prepared 87,000 examples...\n   • Prepared 88,000 examples...\n   • Prepared 89,000 examples...\n   • Prepared 90,000 examples...\n   • Prepared 91,000 examples...\n   • Prepared 92,000 examples...\n   • Prepared 93,000 examples...\n   • Prepared 94,000 examples...\n   • Prepared 95,000 examples...\n   • Prepared 96,000 examples...\n   • Prepared 97,000 examples...\n   • Prepared 98,000 examples...\n   • Prepared 99,000 examples...\n   • Prepared 100,000 examples...\n   • Prepared 101,000 examples...\n   • Prepared 102,000 examples...\n   • Prepared 103,000 examples...\n   • Prepared 104,000 examples...\n   • Prepared 105,000 examples...\n   • Prepared 106,000 examples...\n   • Prepared 107,000 examples...\n   • Prepared 108,000 examples...\n   • Prepared 109,000 examples...\n   • Prepared 110,000 examples...\n   • Prepared 111,000 examples...\n   • Prepared 112,000 examples...\n   • Prepared 113,000 examples...\n   • Prepared 114,000 examples...\n   • Prepared 115,000 examples...\n   • Prepared 116,000 examples...\n   • Prepared 117,000 examples...\n   • Prepared 118,000 examples...\n   • Prepared 119,000 examples...\n","output_type":"stream"},{"name":"stderr","text":"[2025-08-13 16:35:34,718] [INFO] Created vocabulary\n[2025-08-13 16:35:34,719] [INFO] Finished initializing nlp object\n","output_type":"stream"},{"name":"stdout","text":"119,662 training examples prepared\n\nInitializing model...\n   Model initialized successfully\n\nStarting training loop...\n   • Epochs: 10\n   • Batch size: 32\n   • Dropout rate: 0.3\n   • Learning rate: 0.001\n   📊 Epoch  1: Loss=12.3563, Time=63.0s\n   📊 Epoch  2: Loss=3.5135, Accuracy=0.9950 (99.5%), Time=69.4s\n   📊 Epoch  3: Loss=2.7931, Time=75.9s\n   📊 Epoch  4: Loss=2.3963, Accuracy=1.0000 (100.0%), Time=81.2s\n   📊 Epoch  5: Loss=2.0810, Time=84.8s\n   📊 Epoch  6: Loss=1.9643, Accuracy=1.0000 (100.0%), Time=89.6s\n   📊 Epoch  7: Loss=1.8487, Time=93.3s\n   📊 Epoch  8: Loss=1.6160, Accuracy=1.0000 (100.0%), Time=95.9s\n   📊 Epoch  9: Loss=1.8500, Time=102.1s\n   📊 Epoch 10: Loss=1.7227, Accuracy=1.0000 (100.0%), Time=101.1s\nTraining completed successfully!\n\nFinal Model Evaluation...\n   • Running full test set evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating model:   0%|          | 0/29916 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nFINAL EVALUATION RESULTS:\n========================================\nOverall Accuracy: 0.9987 (99.87%)\nTest Samples: 29,916\n\nDetailed Classification Report:\n              precision    recall  f1-score   support\n\n       Human     0.9982    0.9992    0.9987     14958\n          AI     0.9992    0.9982    0.9987     14958\n\n    accuracy                         0.9987     29916\n   macro avg     0.9987    0.9987    0.9987     29916\nweighted avg     0.9987    0.9987    0.9987     29916\n\n\nConfusion Matrix:\n                 Predicted\nActual       Human      AI\nHuman        14946        12\nAI              27     14931\n\n📈 Confidence Statistics:\n   • Mean AI confidence: 0.4995\n   • Mean Human confidence: 0.5005\n   • Std AI confidence: 0.4998\n   • Std Human confidence: 0.4998\n\nSUCCESS! Training completed!\n========================================\nTotal training time: 18.1 minutes\nFinal accuracy: 0.9987 (99.87%)\nModel is ready for use!\nModel saved to: ./spacy_ai_detector_WORKING\n\n🏁 Training process completed at: 16:50:43\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n\nif TRAINING_SUCCESSFUL and TRAINED_MODEL is not None:\n    print(\"🎯 Running sample predictions to verify model works correctly...\")\n    \n    # Test with diverse sample texts\n    test_texts = [\n        # Human-like texts\n        \"I love going to the beach with my family during summer vacation. The sand feels warm between my toes and the waves are so relaxing.\",\n        \"Yesterday I had the worst day at work. My boss was being unreasonable and I spilled coffee on my shirt. Can't wait for the weekend!\",\n        \"My grandmother makes the best apple pie in the world. She uses a secret ingredient that she won't tell anyone, not even me.\",\n        \n        # AI-like texts  \n        \"The implementation of machine learning algorithms requires careful consideration of hyperparameters and model architecture to achieve optimal performance metrics.\",\n        \"According to recent studies, artificial intelligence has demonstrated significant advancements in natural language processing tasks, showing remarkable capabilities in text generation and comprehension.\",\n        \"The utilization of deep learning frameworks enables efficient processing of large-scale datasets, facilitating the development of robust predictive models across various domains.\",\n        \n        # Edge cases\n        \"This is a simple test sentence.\",\n        \"The quick brown fox jumps over the lazy dog and runs through the forest quickly.\"\n    ]\n    \n    print(f\"\\n🔍 Sample Predictions:\")\n    print(\"-\" * 80)\n    \n    for i, text in enumerate(test_texts, 1):\n        doc = TRAINED_MODEL(text)\n        \n        ai_confidence = doc.cats[\"AI\"]\n        human_confidence = doc.cats[\"HUMAN\"]\n        prediction = \"AI\" if ai_confidence > human_confidence else \"Human\"\n        confidence = max(ai_confidence, human_confidence)\n        \n        print(f\"Test {i:2d}: {prediction:5s} (conf: {confidence:.3f}) | {text[:70]}{'...' if len(text) > 70 else ''}\")\n    \n    print(f\"\\nModel testing completed successfully!\")\n    print(f\"The model is making predictions for both AI and Human classes\")\n    print(f\"Final accuracy on test set: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n    \nelse:\n    print(\"Cannot test model - training was not successful\")\n    print(\"Please re-run the training cell to fix any issues\")\n\nprint(f\"\\nSpaCy AI text detector is ready!\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:53:21.960972Z","iopub.execute_input":"2025-08-13T16:53:21.961587Z","iopub.status.idle":"2025-08-13T16:53:21.973324Z","shell.execute_reply.started":"2025-08-13T16:53:21.961561Z","shell.execute_reply":"2025-08-13T16:53:21.972681Z"}},"outputs":[{"name":"stdout","text":"🎯 Running sample predictions to verify model works correctly...\n\n🔍 Sample Predictions:\n--------------------------------------------------------------------------------\nTest  1: Human (conf: 0.571) | I love going to the beach with my family during summer vacation. The s...\nTest  2: Human (conf: 0.740) | Yesterday I had the worst day at work. My boss was being unreasonable ...\nTest  3: Human (conf: 0.908) | My grandmother makes the best apple pie in the world. She uses a secre...\nTest  4: AI    (conf: 0.962) | The implementation of machine learning algorithms requires careful con...\nTest  5: AI    (conf: 0.992) | According to recent studies, artificial intelligence has demonstrated ...\nTest  6: AI    (conf: 0.934) | The utilization of deep learning frameworks enables efficient processi...\nTest  7: AI    (conf: 0.543) | This is a simple test sentence.\nTest  8: AI    (conf: 0.747) | The quick brown fox jumps over the lazy dog and runs through the fores...\n\nModel testing completed successfully!\nThe model is making predictions for both AI and Human classes\nFinal accuracy on test set: 0.9987 (99.87%)\n\nSpaCy AI text detector is ready!\n============================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!zip -r output.zip ./","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:53:32.835010Z","iopub.execute_input":"2025-08-13T16:53:32.835282Z","iopub.status.idle":"2025-08-13T16:53:33.788318Z","shell.execute_reply.started":"2025-08-13T16:53:32.835261Z","shell.execute_reply":"2025-08-13T16:53:33.787617Z"}},"outputs":[{"name":"stdout","text":"  adding: spacy_ai_detector_WORKING/ (stored 0%)\n  adding: spacy_ai_detector_WORKING/config.cfg (deflated 60%)\n  adding: spacy_ai_detector_WORKING/vocab/ (stored 0%)\n  adding: spacy_ai_detector_WORKING/vocab/key2row (stored 0%)\n  adding: spacy_ai_detector_WORKING/vocab/strings.json (deflated 80%)\n  adding: spacy_ai_detector_WORKING/vocab/vectors.cfg (stored 0%)\n  adding: spacy_ai_detector_WORKING/vocab/lookups.bin (stored 0%)\n  adding: spacy_ai_detector_WORKING/vocab/vectors (deflated 45%)\n  adding: spacy_ai_detector_WORKING/tokenizer (deflated 81%)\n  adding: spacy_ai_detector_WORKING/textcat/ (stored 0%)\n  adding: spacy_ai_detector_WORKING/textcat/cfg (deflated 17%)\n  adding: spacy_ai_detector_WORKING/textcat/model (deflated 7%)\n  adding: spacy_ai_detector_WORKING/meta.json (deflated 49%)\n  adding: .virtual_documents/ (stored 0%)\n","output_type":"stream"}],"execution_count":7}]}